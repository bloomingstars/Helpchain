{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  federated learning",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bloomingstars/Helpchain/blob/master/Copy_of_federated_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90r7KEIEAblv",
        "colab_type": "code",
        "outputId": "424a25cc-57e7-4ac8-b0ab-128e10f83f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/92/1d41de2cbb196dc315e083228ad41308107e7a298e7f547106daa756ee0c/syft-0.2.0a2-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 2.7MB/s \n",
            "\u001b[?25hCollecting msgpack>=0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/a8/e01fea81691749044a7bfd44536483a296d9c0a7ed4ec8810a229435547c/msgpack-0.6.2-cp36-cp36m-manylinux1_x86_64.whl (249kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 77.0MB/s \n",
            "\u001b[?25hCollecting lz4>=2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/cedd32c203ce0303188b0c7ff8388bba3c33e4bf6da21ae789962c4fb2e7/lz4-2.2.1-cp36-cp36m-manylinux1_x86_64.whl (395kB)\n",
            "\u001b[K     |████████████████████████████████| 399kB 76.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.3 in /usr/local/lib/python3.6/dist-packages (from syft) (1.3.0+cu100)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.17.3)\n",
            "Collecting websockets>=7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/cb/c35513c4a0ff24ca13e33f7336ba8c1a864449fad9fea8e37abdad11c38d/websockets-8.1-cp36-cp36m-manylinux1_x86_64.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision==0.4.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.4.1+cu100)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.5.0)\n",
            "Collecting zstd>=1.4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/46/c17843364277eef48437c1ad8d083600c82afab23b6bc8281eae247d0277/zstd-1.4.4.0.tar.gz (469kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 34.0MB/s \n",
            "\u001b[?25hCollecting flask-socketio>=3.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 75.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.16.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.1->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.1->syft) (4.3.0)\n",
            "Collecting python-socketio>=4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/b0/22c3f785f23fec5c7a815f47c55d7e7946a67ae2129ff604148e939d3bdb/python_socketio-4.3.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.4.1->syft) (0.46)\n",
            "Collecting python-engineio>=3.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/fb/843f043988306d12f27ae62a82ce26edc7513580e5e089b4911326879f7e/python_engineio-3.10.0-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 78.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: zstd\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zstd: filename=zstd-1.4.4.0-cp36-cp36m-linux_x86_64.whl size=1131209 sha256=d1950e2c0feef109ad9bb094cd5ff05010955ce2f9222dc953b0e4cca6fddf92\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0d/5a/3c2ccf5522d1cd3ecdeb2af8c9889179d7a204376b1d5c042d\n",
            "Successfully built zstd\n",
            "Installing collected packages: msgpack, lz4, websockets, zstd, python-engineio, python-socketio, flask-socketio, websocket-client, syft\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "Successfully installed flask-socketio-4.2.1 lz4-2.2.1 msgpack-0.6.2 python-engineio-3.10.0 python-socketio-4.3.1 syft-0.2.0a2 websocket-client-0.56.0 websockets-8.1 zstd-1.4.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHIef6hAflo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tatdWtN3AqAV",
        "colab_type": "code",
        "outputId": "6451f1d3-3c7d-44aa-9536-afeb33ee3f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import syft as sy \n",
        "import random \n",
        "\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 128\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 1\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 200316905 ## TODO change seed to your studentID inside the class Arguments (line 17)\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def train(args, model, device, federated_train_loader, optimizer, epoch, participates):\n",
        "    model.train()  # <-- initial training\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        if target.location.id in participates:\n",
        "            model.send(data.location) # <-- NEW: send the model to the right location\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            model.get() # <-- NEW: get the model back\n",
        "            if batch_idx % args.log_interval == 0:\n",
        "                loss = loss.get() # <-- NEW: get the loss back\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                    100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "\n",
        "\n",
        "            \n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "### main function\n",
        "\n",
        "args = Arguments()\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "torch.manual_seed(args.seed) \n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "\n",
        "node1 = sy.VirtualWorker(hook, id=\"node1\")\n",
        "node2 = sy.VirtualWorker(hook, id=\"node2\")\n",
        "node3 = sy.VirtualWorker(hook, id=\"node3\")\n",
        "node4 = sy.VirtualWorker(hook, id=\"node4\")\n",
        "node5 = sy.VirtualWorker(hook, id=\"node5\")\n",
        "node6 = sy.VirtualWorker(hook, id=\"node6\")\n",
        "node7 = sy.VirtualWorker(hook, id=\"node7\")\n",
        "node8 = sy.VirtualWorker(hook, id=\"node8\")\n",
        "node9 = sy.VirtualWorker(hook, id=\"node9\")\n",
        "node10 = sy.VirtualWorker(hook, id=\"node10\")\n",
        "\n",
        "##-------------------------------------------\n",
        "\n",
        "## distribute data across nodes\n",
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((node1,node2,node3,node4,node5,node6,node7,node8,node9,node10)), \n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "## test dataset is always same at the central server\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "## training models in a federated appraoch\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) \n",
        "\n",
        "## TODO: select a random set of node ids that will be passed to the training function; these nodes will particiapte in the federated learning\n",
        "full_node_list=['node1','node2','node3','node4','node5','node6','node7','node8','node9','node10']\n",
        "\n",
        "#for varying k\n",
        "for k_val in [3,5,7,10]:\n",
        "    select_nodes=random.sample(full_node_list,k=k_val) #change this\n",
        "    train(args, model, device, federated_train_loader, optimizer, 3,select_nodes ) ## TODO: pass the node_id list like ['node1','node2' ...]\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "for n_val in [3,5,10]:\n",
        "    select_nodes=random.sample(full_node_list,k=5) #change this\n",
        "    train(args, model, device, federated_train_loader, optimizer, n_val,select_nodes ) ## TODO: pass the node_id list like ['node1','node2' ...]\n",
        "    test(args, model, device, test_loader)\n",
        "#create node_list \n",
        "\n",
        "##-------------------------------------------\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, 3,select_nodes ) ## TODO: pass the node_id list like ['node1','node2' ...]\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3975028.77it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 57217.74it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 944710.38it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21713.96it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 2.247151\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 2.048231\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 1.771606\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 1.170154\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.719337\n",
            "\n",
            "Test set: Average loss: 0.5875, Accuracy: 8410/10000 (84%)\n",
            "\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.542484\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.464293\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.467463\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.354850\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.427755\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.335161\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.407956\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.253770\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.248502\n",
            "\n",
            "Test set: Average loss: 0.2811, Accuracy: 9157/10000 (92%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.360972\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.262564\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.384958\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.365349\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.235043\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.335534\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.222039\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.163768\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.295904\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.165156\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.127371\n",
            "\n",
            "Test set: Average loss: 0.1832, Accuracy: 9453/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.157665\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.197893\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.251786\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.121083\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.142765\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.144985\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.187268\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.196403\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.122125\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.077944\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.142260\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.145639\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.167729\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.135868\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.140456\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.170956\n",
            "\n",
            "Test set: Average loss: 0.1288, Accuracy: 9628/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.169203\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.054848\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.119898\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.120878\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.092333\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.177662\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.104379\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.101664\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9658/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.100756\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.137658\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.091472\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.063883\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.085095\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.125722\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.185354\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.181485\n",
            "\n",
            "Test set: Average loss: 0.0963, Accuracy: 9712/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.258668\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.081531\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.060586\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.149975\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.115971\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.171027\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.054464\n",
            "\n",
            "Test set: Average loss: 0.0866, Accuracy: 9748/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.078252\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.052397\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.042477\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.096924\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.118311\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.094470\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.093933\n",
            "\n",
            "Test set: Average loss: 0.0842, Accuracy: 9751/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}